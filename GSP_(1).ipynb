{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mlandvweni/Mlandvwen/blob/main/GSP_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69d46a90-a21b-4e81-8f02-0f791b5bedac",
      "metadata": {
        "id": "69d46a90-a21b-4e81-8f02-0f791b5bedac"
      },
      "source": [
        "This project shows a basic **Natural Language Processing (NLP)** pipeline using 5 sample messages from the **SMS Spam Collection dataset**. The pipeline includes **tokenization**, **stopword removal**, **lemmatization**, and **TF-IDF vectorization**. These steps clean and transform the text into numerical features suitable for further analysis or machine learning tasks like **spam detection**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c732239-324b-4843-ba5d-72bc7bdc983a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c732239-324b-4843-ba5d-72bc7bdc983a",
        "outputId": "205ca85e-9dab-477a-9c26-f2eb0db52e8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text 1: Go until jurong point, crazy.. Available only in bugis n great world la e buffet...\n",
            "Text 2: Ok lar... Joking wif u oni...\n",
            "Text 3: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "Text 4: U dun say so early hor... U c already then say...\n",
            "Text 5: WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n"
          ]
        }
      ],
      "source": [
        "# Load 5 Sample Messagesfrom the SMSSpamCollection dataset\n",
        "texts = [\n",
        "    \"Go until jurong point, crazy.. Available only in bugis n great world la e buffet...\",\n",
        "    \"Ok lar... Joking wif u oni...\",\n",
        "    \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
        "    \"U dun say so early hor... U c already then say...\",\n",
        "    \"WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\"\n",
        "]\n",
        "\n",
        "# Print the messages\n",
        "for i, msg in enumerate(texts):\n",
        "    print(f\"Text {i+1}: {msg}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student number: 1113552\n",
        "\n",
        "Student Name: Zithile\n",
        "\n",
        "Job Breakdown: :Responsible for Tokenization"
      ],
      "metadata": {
        "id": "B8R-9OPwgzBN"
      },
      "id": "B8R-9OPwgzBN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dd0831a-0d55-4a57-8799-87fba87fff40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dd0831a-0d55-4a57-8799-87fba87fff40",
        "outputId": "9b9e2b80-3d27-4a78-d7b8-ddb208259ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens for Text 1:\n",
            "['Go', 'until', 'jurong', 'point,', 'crazy..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...']\n",
            "Tokens for Text 2:\n",
            "['Ok', 'lar...', 'Joking', 'wif', 'u', 'oni...']\n",
            "Tokens for Text 3:\n",
            "['Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005.', 'Text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question(std', 'txt', \"rate)T&C's\", 'apply', \"08452810075over18's\"]\n",
            "Tokens for Text 4:\n",
            "['U', 'dun', 'say', 'so', 'early', 'hor...', 'U', 'c', 'already', 'then', 'say...']\n",
            "Tokens for Text 5:\n",
            "['WINNER!!', 'As', 'a', 'valued', 'network', 'customer', 'you', 'have', 'been', 'selected', 'to', 'receivea', '£900', 'prize', 'reward!', 'To', 'claim', 'call', '09061701461.', 'Claim', 'code', 'KL341.', 'Valid', '12', 'hours', 'only.']\n"
          ]
        }
      ],
      "source": [
        "# Tokenize each message(Tokenization)\n",
        "tokenized_texts = [text.split() for text in texts]\n",
        "\n",
        "# Display tokenized results\n",
        "for i, tokens in enumerate(tokenized_texts):\n",
        "    print(f\"Tokens for Text {i+1}:\")\n",
        "    print(tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Student number:** 1113523\n",
        "\n",
        "**Student Name:** Mlandvo Dlamini\n",
        "\n",
        "**Job Breakdown:** Responcible for Stopword removal"
      ],
      "metadata": {
        "id": "xYU20N6Sgjut"
      },
      "id": "xYU20N6Sgjut"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NeZt0w2KXTMr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeZt0w2KXTMr",
        "outputId": "9a1b381d-bb92-47b8-da88-72a27569db0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Tokens for Text 1:\n",
            "['Go', 'jurong', 'point,', 'crazy..', 'Available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...']\n",
            "Filtered Tokens for Text 2:\n",
            "['Ok', 'lar...', 'Joking', 'wif', 'u', 'oni...']\n",
            "Filtered Tokens for Text 3:\n",
            "['Free', 'entry', '2', 'wkly', 'comp', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005.', 'Text', 'FA', '87121', 'receive', 'entry', 'question(std', 'txt', \"rate)T&C's\", 'apply', \"08452810075over18's\"]\n",
            "Filtered Tokens for Text 4:\n",
            "['U', 'dun', 'say', 'early', 'hor...', 'U', 'c', 'already', 'say...']\n",
            "Filtered Tokens for Text 5:\n",
            "['WINNER!!', 'valued', 'network', 'customer', 'selected', 'receivea', '£900', 'prize', 'reward!', 'claim', 'call', '09061701461.', 'Claim', 'code', 'KL341.', 'Valid', '12', 'hours', 'only.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Stopword Removal\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from each tokenized message\n",
        "filtered_texts = []\n",
        "for tokens in tokenized_texts:\n",
        "    filtered = [word for word in tokens if word.lower() not in stop_words]\n",
        "    filtered_texts.append(filtered)\n",
        "\n",
        "# Display filtered (stopwords removed) tokens\n",
        "for i, tokens in enumerate(filtered_texts):\n",
        "    print(f\"Filtered Tokens for Text {i+1}:\")\n",
        "    print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gLn5VINNXYVH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLn5VINNXYVH",
        "outputId": "9802df55-a8bd-4159-c301-51ea41a15de3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lemmatized Tokens for Text 1:\n",
            "['go', 'jurong', 'point,', 'crazy..', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...']\n",
            "Lemmatized Tokens for Text 2:\n",
            "['ok', 'lar...', 'joking', 'wif', 'u', 'oni...']\n",
            "Lemmatized Tokens for Text 3:\n",
            "['free', 'entry', '2', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005.', 'text', 'fa', '87121', 'receive', 'entry', 'question(std', 'txt', \"rate)t&c's\", 'apply', \"08452810075over18's\"]\n",
            "Lemmatized Tokens for Text 4:\n",
            "['u', 'dun', 'say', 'early', 'hor...', 'u', 'c', 'already', 'say...']\n",
            "Lemmatized Tokens for Text 5:\n",
            "['winner!!', 'valued', 'network', 'customer', 'selected', 'receivea', '£900', 'prize', 'reward!', 'claim', 'call', '09061701461.', 'claim', 'code', 'kl341.', 'valid', '12', 'hour', 'only.']\n"
          ]
        }
      ],
      "source": [
        "# Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize the filtered tokens\n",
        "lemmatized_texts = []\n",
        "for tokens in filtered_texts:\n",
        "    lemmatized = [lemmatizer.lemmatize(word.lower()) for word in tokens]\n",
        "    lemmatized_texts.append(lemmatized)\n",
        "\n",
        "# Display lemmatized tokens\n",
        "for i, tokens in enumerate(lemmatized_texts):\n",
        "    print(f\"Lemmatized Tokens for Text {i+1}:\")\n",
        "    print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87be9b73",
      "metadata": {
        "id": "87be9b73"
      },
      "source": [
        "**Student number**: 1103558\n",
        "\n",
        "**Student Name:** Mikollito Ong\n",
        "\n",
        "**Job Breakdown:** : Responsible for TF-IDF Cectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rJ-aDhpoXecD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ-aDhpoXecD",
        "outputId": "9cac1fe1-46ae-477f-9aaa-527ccb0edbd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Feature Names:\n",
            "['08452810075over18' '09061701461' '12' '2005' '21st' '87121' '900'\n",
            " 'already' 'apply' 'available' 'buffet' 'bugis' 'call' 'claim' 'code'\n",
            " 'comp' 'crazy' 'cup' 'customer' 'dun' 'early' 'entry' 'fa' 'final' 'free'\n",
            " 'go' 'great' 'hor' 'hour' 'joking' 'jurong' 'kl341' 'la' 'lar' 'may'\n",
            " 'network' 'ok' 'oni' 'only' 'point' 'prize' 'question' 'rate' 'receive'\n",
            " 'receivea' 'reward' 'say' 'selected' 'std' 'text' 'tkts' 'txt' 'valid'\n",
            " 'valued' 'wif' 'win' 'winner' 'wkly' 'world']\n",
            "\n",
            "TF-IDF Matrix Shape: (5, 59)\n",
            "\n",
            "TF-IDF Values:\n",
            "   08452810075over18  09061701461     12   2005   21st  87121    900  already  \\\n",
            "0              0.000        0.000  0.000  0.000  0.000  0.000  0.000    0.000   \n",
            "1              0.000        0.000  0.000  0.000  0.000  0.000  0.000    0.000   \n",
            "2              0.192        0.000  0.000  0.192  0.192  0.192  0.000    0.000   \n",
            "3              0.000        0.000  0.000  0.000  0.000  0.000  0.000    0.354   \n",
            "4              0.000        0.218  0.218  0.000  0.000  0.000  0.218    0.000   \n",
            "\n",
            "   apply  available  ...   text   tkts    txt  valid  valued    wif    win  \\\n",
            "0  0.000      0.316  ...  0.000  0.000  0.000  0.000   0.000  0.000  0.000   \n",
            "1  0.000      0.000  ...  0.000  0.000  0.000  0.000   0.000  0.447  0.000   \n",
            "2  0.192      0.000  ...  0.192  0.192  0.192  0.000   0.000  0.000  0.192   \n",
            "3  0.000      0.000  ...  0.000  0.000  0.000  0.000   0.000  0.000  0.000   \n",
            "4  0.000      0.000  ...  0.000  0.000  0.000  0.218   0.218  0.000  0.000   \n",
            "\n",
            "   winner   wkly  world  \n",
            "0   0.000  0.000  0.316  \n",
            "1   0.000  0.000  0.000  \n",
            "2   0.000  0.192  0.000  \n",
            "3   0.000  0.000  0.000  \n",
            "4   0.218  0.000  0.000  \n",
            "\n",
            "[5 rows x 59 columns]\n"
          ]
        }
      ],
      "source": [
        "#TF-IDF Vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Join lemmatized tokens back into full strings\n",
        "final_texts = [' '.join(tokens) for tokens in lemmatized_texts]\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(final_texts)\n",
        "\n",
        "# Display TF-IDF feature names\n",
        "print(\"TF-IDF Feature Names:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display the TF-IDF matrix shape\n",
        "print(\"\\nTF-IDF Matrix Shape:\", tfidf_matrix.shape)\n",
        "\n",
        "# Optional: Show TF-IDF values as a dense matrix\n",
        "import pandas as pd\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Values:\")\n",
        "print(df_tfidf.round(3))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}